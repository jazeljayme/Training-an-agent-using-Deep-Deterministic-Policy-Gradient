{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-02T03:16:18.580796Z",
     "start_time": "2022-03-02T03:16:18.576658Z"
    },
    "id": "uwqde8HtE3LI"
   },
   "outputs": [],
   "source": [
    "# !pip3 install box2d-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-02T03:16:22.985950Z",
     "start_time": "2022-03-02T03:16:18.584873Z"
    },
    "id": "wKW_u2ZQ-u4K"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/msds2022/jjayme/.conda/envs/msds-ml3-rl/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-02T03:16:23.037266Z",
     "start_time": "2022-03-02T03:16:22.988287Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "This environment is a continuous control tasks in the Box2D simulator. The goal is to train an agent to control the landing of a rocket into a landing pad. In this environment, landing outside the landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt. The environment is found in this [link](https://gym.openai.com/envs/LunarLander-v2/).\n",
    "\n",
    "### Actions\n",
    "The agent has to decide between four actions:\n",
    "    Action space (Discrete)\n",
    "- 0- Do nothing\n",
    "- 1- Fire left engine\n",
    "- 2- Fire down engine\n",
    "- 3- Fire right engine\n",
    "\n",
    "### States\n",
    "The state of the lander is encoded in 8 variables:\n",
    "- x position\n",
    "- y position\n",
    "- x velocity\n",
    "- y velocity\n",
    "- angle\n",
    "- angular velocity\n",
    "- left leg touching ground\n",
    "- right leg touching ground\n",
    "\n",
    "### Rewards\n",
    "As the agent observes the current state of the environment and chooses\n",
    "an action, the environment *transitions* to a new state, and also\n",
    "returns a reward that indicates the consequences of the action.\n",
    "This environment rewards the agent for the following:\n",
    "- -100 lander crashed or lands outside landing pad (ends an episode)\n",
    "- +100 lander comes to rest within landing pad (ends an episode)\n",
    "- +10 for each leg currently on the ground (lifting a leg incurs a -10 reward)\n",
    "- -0.3 for each frame the main engine is used\n",
    "- -0.03 for using the side engines\n",
    "- There are miscellaneous positive (negative) rewards for decreasing (increasing) the distance to the landing pads.\n",
    "\n",
    "The rewards incentivise the agent for landing inside the landing pad on both legs, while using the least amount of fuel as possible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning using DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DQN algorithm implemented on this notebook was copied from Doc. Damian's DQN architecture discussed in class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-02T03:16:23.066975Z",
     "start_time": "2022-03-02T03:16:23.039791Z"
    },
    "id": "PuqZyA9MAqc-"
   },
   "outputs": [],
   "source": [
    "# intializing\n",
    "seed = 0\n",
    "\n",
    "# load the environment from openai gym\n",
    "env = gym.make(\"LunarLander-v2\").env\n",
    "env.seed(seed)\n",
    "state = env.reset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-02T03:16:23.090931Z",
     "start_time": "2022-03-02T03:16:23.069287Z"
    },
    "id": "euW9mpG4FILZ"
   },
   "outputs": [],
   "source": [
    "def create_q_model(num_observations, num_actions):\n",
    "    inputs = layers.Input(shape=(num_observations))\n",
    "    layer1 = layers.Dense(64, activation=\"relu\")(inputs)\n",
    "    layer2 = layers.Dense(64, activation=\"relu\")(layer1)\n",
    "    action = layers.Dense(num_actions, activation=None)(layer2)\n",
    "    return keras.Model(inputs=inputs, outputs=action)\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append((*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        samples = random.sample(self.memory, batch_size)\n",
    "        action_sample = [sample[0] for sample in samples]\n",
    "        state_sample = np.array([sample[1] for sample in samples])\n",
    "        state_next_sample = np.array([sample[2] for sample in samples])\n",
    "        rewards_sample = [sample[3] for sample in samples]\n",
    "        done_sample = tf.convert_to_tensor([float(sample[4]) for sample in samples])\n",
    "\n",
    "        return (\n",
    "            action_sample,\n",
    "            state_sample,\n",
    "            state_next_sample,\n",
    "            rewards_sample,\n",
    "            done_sample,\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class Agent:\n",
    "    \"Interacts with the environment\"\n",
    "\n",
    "    def __init__(self, num_observations, num_actions):\n",
    "        self.num_observations = num_observations\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "        # The first model makes the predictions for Q-values which are used to make a action.\n",
    "        self.model_policy = create_q_model(num_observations, num_actions)\n",
    "        self.model_target = create_q_model(num_observations, num_actions)\n",
    "        # Deepmind paper used RMSProp however then Adam optimizer is faster\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        self.memory = ReplayBuffer(buffer_size)\n",
    "        self.step_count = 0\n",
    "\n",
    "    def step(self, action, state, state_next, reward, done):\n",
    "        # Save actions and states in replay buffer\n",
    "        self.memory.push((action, state, state_next, reward, done))\n",
    "\n",
    "        self.step_count += 1\n",
    "        # Update every `train_freq` frame if `batch_size` samples available\n",
    "        if self.step_count % train_freq == 0 and len(self.memory) > batch_size:\n",
    "            # sample the replay buffer\n",
    "            experience_sample = self.memory.sample(batch_size)\n",
    "            self.learn(experience_sample)\n",
    "\n",
    "        if self.step_count % update_target_network == 0:\n",
    "            # update the the target network with new weights\n",
    "            self.model_target.set_weights(self.model_policy.get_weights())\n",
    "\n",
    "    def act(self, state, eps=0):\n",
    "        # Use epsilon-greedy for exploration\n",
    "        if epsilon > np.random.random():\n",
    "            # Take random action\n",
    "            action = np.random.choice(self.num_actions)\n",
    "        else:\n",
    "            # Predict action Q-values from state\n",
    "            action_probs = self.model_policy(state[np.newaxis], training=False)\n",
    "            # Take best action\n",
    "            action = tf.argmax(action_probs[0]).numpy()\n",
    "        return action\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        loss_function = keras.losses.Huber()  # Using huber loss for stability\n",
    "\n",
    "        (\n",
    "            action_sample,\n",
    "            state_sample,\n",
    "            state_next_sample,\n",
    "            rewards_sample,\n",
    "            done_sample,\n",
    "        ) = experiences\n",
    "        # Build the updated Q-values for the sampled future states\n",
    "        # Use the target model for stability\n",
    "        future_rewards = self.model_target.predict(state_next_sample)\n",
    "        # Q value = reward + discount factor * expected future reward\n",
    "        updated_q_values = rewards_sample + gamma * tf.reduce_max(\n",
    "            future_rewards, axis=1\n",
    "        ) * (1 - done_sample)\n",
    "        # final frame has no future reward\n",
    "\n",
    "        # Create a mask so we only calculate loss on the updated Q-values\n",
    "        masks = tf.one_hot(action_sample, self.num_actions)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Train the model on the states and updated Q-values\n",
    "            q_values = self.model_policy(state_sample)\n",
    "\n",
    "            # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "            q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "            # Calculate loss between new Q-value and old Q-value\n",
    "            loss = loss_function(updated_q_values, q_action)\n",
    "\n",
    "        # Backpropagation\n",
    "        grads = tape.gradient(loss, self.model_policy.trainable_variables)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(grads, self.model_policy.trainable_variables)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-02T11:43:56.812096Z",
     "start_time": "2022-03-02T03:17:00.334378Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "_sI6tDpsFitO",
    "outputId": "7d14c41f-cd8d-46cb-abae-bc5fc7b2bfe1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4992/1000000 [01:30<5:11:37, 53.22it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 271392/1000000 [1:37:54<6:12:14, 32.62it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 30%|██▉       | 299863/1000000 [1:59:34<10:39:25, 18.25it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 32%|███▏      | 317866/1000000 [2:22:05<18:26:47, 10.27it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 33%|███▎      | 331249/1000000 [2:45:41<22:56:54,  8.09it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 36%|███▋      | 364409/1000000 [3:08:16<11:02:54, 15.98it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 40%|███▉      | 399996/1000000 [3:28:12<4:27:01, 37.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 401738/1000000 [3:28:58<5:06:41, 32.51it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 50%|████▉     | 499992/1000000 [4:25:10<3:35:19, 38.70it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 599996/1000000 [5:08:20<3:07:52, 35.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 699996/1000000 [5:53:48<2:16:55, 36.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 799999/1000000 [6:46:26<2:57:05, 18.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 899996/1000000 [7:40:37<48:09, 34.61it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 999999/1000000 [8:26:55<00:00, 34.53it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [8:26:55<00:00, 32.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# Configuration paramaters for the whole setup\n",
    "gamma = 0.99  # Discount factor for past rewards\n",
    "epsilon_min = 0.05  # Minimum epsilon greedy parameter\n",
    "epsilon_max = 1.0  # Maximum epsilon greedy parameter\n",
    "epsilon = epsilon_max  # Epsilon greedy parameter\n",
    "batch_size = 256  # Size of batch taken from replay buffer\n",
    "max_steps_per_episode = 1000  # just a safety constraint\n",
    "exploration_fraction = 0.6  # Fraction of frames for exploration\n",
    "buffer_size = int(1e6)  # Maximum replay length\n",
    "train_freq = 4  # Train the model after 4 actions\n",
    "update_target_network = 200  # How often to update the target network\n",
    "\n",
    "\n",
    "episode_rewards = [0.0]\n",
    "episode_running_mean_list = []\n",
    "episode_list = []\n",
    "frame_count_list = []\n",
    "\n",
    "num_timesteps = 1_000_000  # longer to train\n",
    "epsilon_greedy_frames = num_timesteps * exploration_fraction\n",
    "\n",
    "agent = Agent(num_observations=8, num_actions=4)\n",
    "state = env.reset()\n",
    "step_count = 0\n",
    "for frame_count in tqdm(range(1, num_timesteps + 1)):\n",
    "    action = agent.act(state, epsilon)\n",
    "\n",
    "    # Apply the sampled action in our environment\n",
    "    state_next, reward, done, _ = env.step(action)\n",
    "    \n",
    "    agent.step(action, state, state_next, reward, done)\n",
    "    state = state_next\n",
    "    episode_rewards[-1] += reward\n",
    "\n",
    "    # Linear Decay probability of taking random action\n",
    "    epsilon -= (epsilon_max - epsilon_min) / epsilon_greedy_frames\n",
    "    epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "    # Log details\n",
    "    if frame_count % (5000) == 0:\n",
    "        running_mean_reward = np.mean(episode_rewards[-20:])\n",
    "        episodes = len(episode_rewards)\n",
    "        episode_running_mean_list.append(running_mean_reward)\n",
    "        episode_list.append(episodes)\n",
    "        frame_count_list.append(frame_count)\n",
    "\n",
    "        df = pd.DataFrame([episode_running_mean_list, episode_list, frame_count_list]).T\n",
    "        df.columns = ['episode_running_mean_list', 'episode_list', 'frame_count_list']\n",
    "\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        episode_rewards.append(0)\n",
    "\n",
    "\n",
    "    if frame_count in [5000, 300_000, 400_000, 500_000, 600_000, 700_000, 800_000, 900_000, 1000_000]:\n",
    "        agent.model_policy.save('models/dqn_'+str(frame_count)+\".h5\")\n",
    "        rewards_done = pd.DataFrame(episode_rewards)\n",
    "        df.to_csv('models/dqn_mean_rewards_'+str(frame_count)+'.csv')\n",
    "        rewards_done.to_csv('models/dqn_episode_rewards_'+str(frame_count)+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-02T16:18:21.251025Z",
     "start_time": "2022-03-02T16:18:21.238285Z"
    },
    "id": "dJcfz0u2HvHD"
   },
   "outputs": [],
   "source": [
    "def evaluate_policy_dqn(policy, env_name, seed, eval_episodes=10, render=False):\n",
    "    eval_env = gym.make(env_name)\n",
    "    eval_env.seed(seed + 100)\n",
    "    avg_reward = 0.\n",
    "    for _ in range(eval_episodes):\n",
    "        state, done = eval_env.reset(), False\n",
    "        while not done:\n",
    "            action_probs = model_policy(state[np.newaxis], training=False)\n",
    "            action = tf.argmax(action_probs[0]).numpy()\n",
    "            if render:\n",
    "                eval_env.render()\n",
    "            state, reward, done, _ = eval_env.step(action)\n",
    "            avg_reward += reward\n",
    "    avg_reward /= eval_episodes\n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-02T16:23:19.231765Z",
     "start_time": "2022-03-02T16:23:15.003795Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82.97793510116728"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DQN\n",
    "env_name = \"LunarLander-v2\"\n",
    "seed = 0\n",
    "model_policy = keras.models.load_model(\"models/dqn_1000000.h5\", compile=False)\n",
    "evaluate_policy_dqn(model_policy, env_name, seed, eval_episodes=10, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:.conda-msds-ml3-rl]",
   "language": "python",
   "name": "conda-env-.conda-msds-ml3-rl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
